{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zhnRZ_rVSAF5",
        "outputId": "3519f1fb-408a-4058-bd4c-73bd1f54531b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-590f2a6901df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtokenize_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtokenize_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0misolation_forest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misolation_forest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tokenize_dataset'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from tokenize_dataset import tokenize_dataset\n",
        "from isolation_forest import isolation_forest\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmljDX4xd2AB"
      },
      "source": [
        "Define dependent variables to prevent overfitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gMil5m1daP0"
      },
      "outputs": [],
      "source": [
        "duke_dependent = ['Surgery', 'Days to Surgery (from the date of diagnosis)', 'Definitive Surgery Type', 'Clinical Response, Evaluated Through Imaging ', 'Pathologic Response to Neoadjuvant Therapy', 'Days to local recurrence (from the date of diagnosis) ', 'Days to distant recurrence(from the date of diagnosis) ', 'Days to death (from the date of diagnosis) ',\n",
        "                            'Days to last local recurrence free assessment (from the date of diagnosis) ', 'Days to last distant recurrence free assemssment(from the date of diagnosis) ', 'Neoadjuvant Chemotherapy', 'Adjuvant Chemotherapy', 'Neoadjuvant Endocrine Therapy Medications ',\n",
        "                            'Adjuvant Endocrine Therapy Medications ', 'Therapeutic or Prophylactic Oophorectomy as part of Endocrine Therapy ', 'Neoadjuvant Anti-Her2 Neu Therapy', 'Adjuvant Anti-Her2 Neu Therapy ', 'Received Neoadjuvant Therapy or Not', 'Pathologic response to Neoadjuvant therapy: Pathologic stage (T) following neoadjuvant therapy ',\n",
        "                            'Pathologic response to Neoadjuvant therapy:  Pathologic stage (N) following neoadjuvant therapy', 'Pathologic response to Neoadjuvant therapy:  Pathologic stage (M) following neoadjuvant therapy ', 'Overall Near-complete Response:  Stricter Definition', 'Overall Near-complete Response:  Looser Definition', 'Near-complete Response (Graded Measure)']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4QVWNrmd-zj"
      },
      "source": [
        "Set target variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bncm7n2leB4Z"
      },
      "outputs": [],
      "source": [
        "target = 'Surgery'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsqIyOwBV3tx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Cancer ML Clinical Data/Clinical and Other Features (edited).csv\", low_memory=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh2L948bWqGT"
      },
      "outputs": [],
      "source": [
        "clinical_ids = df[list(df.columns)[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehpJGDSkW9ys"
      },
      "source": [
        "Set patient IDs as the index of df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tBY_6xWW4rE"
      },
      "outputs": [],
      "source": [
        "df = df.set_index(str(list(df.columns)[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXnXXvD0XDED"
      },
      "outputs": [],
      "source": [
        "df = tokenize_dataset(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5xox1RAf6E7"
      },
      "source": [
        "Partition data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMf2o08FdCjJ"
      },
      "outputs": [],
      "source": [
        "x = df.drop(duke_dependent, axis=1)\n",
        "y = df[target]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eK6my0OIebmR"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=84)\n",
        "\n",
        "X_test, X_val = train_test_split(X_test, test_size=0.5, random_state=84)\n",
        "y_test, y_val = train_test_split(y_test, test_size=0.5, random_state=84)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9ZNe8Nqf_M7"
      },
      "source": [
        "Normalize Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0T6PS0o_fKEp"
      },
      "outputs": [],
      "source": [
        "min_max_scaler = MinMaxScaler()\n",
        "X_train = min_max_scaler.fit_transform(X_train)\n",
        "X_test = min_max_scaler.fit_transform(X_test)\n",
        "X_val = min_max_scaler.fit_transform(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKXiyVK-gqKw"
      },
      "source": [
        "Use Isolation Forest to detect and remove outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3K1E35frgtQ8"
      },
      "outputs": [],
      "source": [
        "predicted = isolation_forest(X_train, y_train)\n",
        "non_outlier_indices = []\n",
        "i = 0\n",
        "for prediction in predicted:\n",
        "  if prediction != -1:\n",
        "    non_outlier_indices.append(i)\n",
        "  i = i + 1\n",
        "\n",
        "num_outliers = len(predicted) - len(non_outlier_indices)\n",
        "print(\"Num Outliers:\", num_outliers)\n",
        "\n",
        "X_train = X_train[non_outlier_indices]\n",
        "y_train = y_train.iloc[non_outlier_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB-1f6RGsBB_"
      },
      "source": [
        "Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFaU3j_vi3sK"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class duke_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(51, 32)\n",
        "        self.fc2 = nn.Linear(32, 16)\n",
        "        self.fc3 = nn.Linear(16, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uWPKVktolWF"
      },
      "outputs": [],
      "source": [
        "model = duke_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy20Myt4ndzt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "from metrics import recall_m, precision_m, f1_m\n",
        "\n",
        "def train_model(config, data, epochs=10, batch_size=32):\n",
        "  X_train = data[0]\n",
        "  y_train = data[1]\n",
        "  X_val = data[2]\n",
        "  y_val = data[3]\n",
        "  y_train = y_train.to_numpy()\n",
        "  y_val = y_val.to_numpy()\n",
        "  X_val = torch.from_numpy(X_val).type(torch.float)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for i in range((X_train.shape[0]-1)//batch_size + 1):\n",
        "      start_i = i*batch_size\n",
        "      end_i = start_i+batch_size\n",
        "      xb = torch.from_numpy(X_train[start_i:end_i]).type(torch.float)\n",
        "      yb = torch.from_numpy(y_train[start_i:end_i])\n",
        "      \n",
        "      pred = model(xb)\n",
        "      loss = criterion(pred, yb)\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "      pred = model(X_val).detach().numpy().astype(np.float)\n",
        "      y_val = torch.from_numpy(y_val)\n",
        "      pred = np.argmax(pred, axis=1)\n",
        "      accuracy = accuracy_score(y_val, pred)\n",
        "      f1_score = f1_m(y_val, pred)\n",
        "      recall = recall_m(y_val, pred)\n",
        "      balanced_acc = balanced_accuracy_score(y_val, pred)\n",
        "      print('Completed training batch', epoch, 'Training Loss is: %.4f' %running_loss, 'Accuracy: %.4f' %accuracy, 'F1: %.4f' %f1_score, 'Recall: %.4f' %recall, 'Balanced Accuracy: %.4f' %balanced_acc)\n",
        "      running_loss = 0.0\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt_I8ECRsCnG"
      },
      "source": [
        "Use Raytune to determine hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38FHL_oGr8Bl"
      },
      "outputs": [],
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from ray.tune.schedulers.async_hyperband import ASHAScheduler\n",
        "\n",
        "config = {\n",
        "    'epochs':tune.choice([50, 100, 150]),\n",
        "    'batch_size':tune.choice([8, 16, 32, 64]),\n",
        "    'lr':tune.loguniform(1e-3, 1e-1),\n",
        "}\n",
        "scheduler = ASHAScheduler(\n",
        "    max_t=10,\n",
        "    grace_period=1,\n",
        "    reduction_factor=3\n",
        ")\n",
        "result = tune.run(\n",
        "    tune.with_parameters(train_model, data=[X_train, y_train, X_val, y_val]),\n",
        "    resources_per_trial={\"cpu\":2},\n",
        "    config=config,\n",
        "    metric=\"loss\",\n",
        "    mode=\"min\",\n",
        "    num_samples=10,\n",
        "    scheduler=scheduler\n",
        ")\n",
        "best_trial = result.get_best_trail(\"loss\", \"min\", \"last\")\n",
        "print(\"Best trial config: {}\".format(best_trial.config))\n",
        "print(\"Best trial final validation loss: {}\".format(\n",
        "    best_trial.last_result[\"loss\"]))\n",
        "print(\"Best trial final validation accuracy: {}\".format(\n",
        "    best_trial.last_result['accuracy']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jk8odc2swz6"
      },
      "outputs": [],
      "source": [
        "train_model(config=best_trial, data=[X_train, y_train, X_val, y_val])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ClinicalOnly.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}